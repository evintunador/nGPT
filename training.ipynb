{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "# running it shouldn't break anything for u\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv')\n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model config\n",
    "from config import *\n",
    "\n",
    "# importing N-GPT\n",
    "from model import cosine_norm, Model\n",
    "\n",
    "# imports for the tokenizer\n",
    "import pickle\n",
    "from tokenizer.tokenizer import BPE_Tokenizer\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "import math\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=128, device=None, max_seq_len=384, theta=10000, vocab_len=2048, num_layers=8, num_heads=4, mlp_hidden_mult=4)\n",
      "TrainConfig(model_name='N-GPT_2m', micro_batch_size=4, grad_accum_steps=16, max_iters=1000, eval_interval=100, beta1=0.9, beta2=0.95, epsilon=1e-08, lr_init=0.01, lr_final=1e-06)\n"
     ]
    }
   ],
   "source": [
    "cfg = ModelConfig()\n",
    "print(cfg)\n",
    "tcfg = TrainConfig()\n",
    "print(tcfg)\n",
    "\n",
    "# size options are 512, 1024 and 2048\n",
    "with open(f'tokenizer/models/{cfg.vocab_len - 3}.model', 'rb') as f:\n",
    "        tokenizer_data = pickle.load(f)\n",
    "tokenizer = BPE_Tokenizer(tokenizer_data['merges']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1844560 parameters\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (s_qk): Scale()\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (a_A): Scale()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=128, out_features=341, bias=False)\n",
      "        (Wgate): Linear(in_features=128, out_features=341, bias=False)\n",
      "        (Wdown): Linear(in_features=341, out_features=128, bias=False)\n",
      "        (s_u): Scale()\n",
      "        (s_v): Scale()\n",
      "      )\n",
      "      (a_M): Scale()\n",
      "    )\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=2048, bias=False)\n",
      "  (s_z): Scale()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(f'{model.get_num_params()} parameters')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7629a97c-5898-4dba-b602-fd26721ff5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9dc2aa7-6a7a-4723-90b9-d00f9ea03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - cfg.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+cfg.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+cfg.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 3): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, target_token_ids=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=tcfg.lr_init, weight_decay=0.0)\n",
    "    # No weight decay to keep vectors on the unit hypersphere\n",
    "\n",
    "# Learning rate schedule without warmup\n",
    "def lr_lambda(current_iter):\n",
    "    # Cosine decay phase only\n",
    "    cosine_decay = 0.5 * (1 + math.cos(math.pi * current_iter / tcfg.max_iters))\n",
    "    return max(cosine_decay, tcfg.lr_final / tcfg.lr_init)\n",
    "        \n",
    "# Scheduler using cosine decay\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.010000, train loss 39.7647, val loss 39.2048, time elapsed: 0.00 seconds\n",
      "step 0100: lr 0.009755, train loss 6.4032, val loss 6.4375, time elapsed: 946.62 seconds\n",
      "step 0200: lr 0.009045, train loss 6.4600, val loss 6.4801, time elapsed: 2147.27 seconds\n",
      "step 0300: lr 0.007939, train loss 6.4539, val loss 6.4991, time elapsed: 5540.19 seconds\n",
      "step 0400: lr 0.006545, train loss 6.4463, val loss 6.4022, time elapsed: 11851.67 seconds\n",
      "step 0500: lr 0.005000, train loss 6.3983, val loss 6.4265, time elapsed: 19312.38 seconds\n",
      "step 0600: lr 0.003455, train loss 6.3908, val loss 6.4396, time elapsed: 28804.56 seconds\n",
      "step 0700: lr 0.002061, train loss 6.4374, val loss 6.4392, time elapsed: 36638.64 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# adding the micro batch's loss to the total loss\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     loss_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 29\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# update the parameters\u001b[39;00m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for iter in range(tcfg.max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % tcfg.eval_interval == 0 or iter == tcfg.max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, tcfg.micro_batch_size)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "    # setup for training\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # we can simulate a larget batch size by accumulating gradients over many micro batches\n",
    "    for micro_step in range(tcfg.grad_accum_steps):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', tcfg.micro_batch_size)\n",
    "        \n",
    "        # train\n",
    "        logits, loss = model(input_token_ids = xb, target_token_ids = yb)\n",
    "        \n",
    "        # accounting for the size of the micro batch\n",
    "        loss = loss / tcfg.grad_accum_steps\n",
    "        # adding the micro batch's loss to the total loss\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        \n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Apply cosine normalization & absolute value constraints after optimization step\n",
    "    model.enforce_constraints()  \n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c7336-62fb-4d0d-a3da-58378d65c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to make sure the absolute value-ing worked\n",
    "print(model.layers[0].a_A.s.data)\n",
    "# checking to make sure the cosine normalization worked\n",
    "print(model.layers[0].mlp.Wup.weight.norm(dim=1))\n",
    "print(model.token_embedder.weight.norm(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import generate\n",
    "output = generate(\n",
    "    \"JULIET:\\nO Romeo, Romeo! wherefore art thou\", \n",
    "    model, \n",
    "    tokenizer, \n",
    "    temperature=1.7, # really weird that we've gotta use a pretty damn low temperature\n",
    "    max_gen_len = 128\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'models/{tcfg.model_name}', exist_ok=True)\n",
    "\n",
    "# saving model\n",
    "torch.save(model.state_dict(), f'models/{tcfg.model_name}/model.pth')\n",
    "\n",
    "# saving configs\n",
    "cfg_dict = asdict(cfg)\n",
    "with open(f'models/{tcfg.model_name}/model_config.json', 'w') as f:\n",
    "    json.dump(cfg_dict, f)\n",
    "tcfg_dict = asdict(tcfg)\n",
    "with open(f'models/{tcfg.model_name}/train_config.json', 'w') as f:\n",
    "    json.dump(tcfg_dict, f)\n",
    "\n",
    "print(f'model successfully saved to models/{tcfg.model_name}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
