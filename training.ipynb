{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "# running it shouldn't break anything for u\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model config\n",
    "from config import *\n",
    "\n",
    "# importing N-GPT\n",
    "from model import cosine_norm, Model\n",
    "\n",
    "# imports for the tokenizer\n",
    "import pickle\n",
    "from tokenizer.tokenizer import BPE_Tokenizer\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "import math\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=96, device='mps', max_seq_len=256, theta=10000, vocab_len=2048, num_layers=8, num_heads=4, mlp_hidden_mult=4)\n",
      "TrainConfig(model_name='N-GPT_1m', micro_batch_size=16, grad_accum_steps=4, max_iters=1000, eval_interval=50, beta1=0.9, beta2=0.95, epsilon=1e-08, lr_init=0.005, lr_final=1e-05)\n"
     ]
    }
   ],
   "source": [
    "cfg = ModelConfig()\n",
    "print(cfg)\n",
    "tcfg = TrainConfig()\n",
    "print(tcfg)\n",
    "\n",
    "# size options are 512, 1024 and 2048\n",
    "with open(f'tokenizer/models/{cfg.vocab_len - 3}.model', 'rb') as f:\n",
    "        tokenizer_data = pickle.load(f)\n",
    "tokenizer = BPE_Tokenizer(tokenizer_data['merges']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089792 parameters\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=96, out_features=96, bias=False)\n",
      "        (Wk): Linear(in_features=96, out_features=96, bias=False)\n",
      "        (Wv): Linear(in_features=96, out_features=96, bias=False)\n",
      "        (Wo): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=96, out_features=256, bias=False)\n",
      "        (Wgate): Linear(in_features=96, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=96, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output): Linear(in_features=96, out_features=2048, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(f'{model.get_num_params()} parameters')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7629a97c-5898-4dba-b602-fd26721ff5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(text[:\u001b[38;5;241m200\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train and test splits\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(text), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     10\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)) \u001b[38;5;66;03m# first 90% will be our training dataset, the rest for validation\u001b[39;00m\n\u001b[1;32m     11\u001b[0m train_data \u001b[38;5;241m=\u001b[39m data[:n]\n",
      "File \u001b[0;32m~/Documents/repos/nGPT/tokenizer/tokenizer.py:40\u001b[0m, in \u001b[0;36mBPE_Tokenizer.encode\u001b[0;34m(self, text, bos, eos, pad)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# nothing else can be merged\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerges[pair]\n\u001b[0;32m---> 40\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge(tokens, pair, idx)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bos:\n\u001b[1;32m     43\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_id] \u001b[38;5;241m+\u001b[39m tokens\n",
      "File \u001b[0;32m~/Documents/repos/nGPT/tokenizer/tokenizer.py:71\u001b[0m, in \u001b[0;36mBPE_Tokenizer.merge\u001b[0;34m(self, ids, pair, idx)\u001b[0m\n\u001b[1;32m     69\u001b[0m newids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# if not at the very last position AND the pair matches, replace it\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids[i] \u001b[38;5;241m==\u001b[39m pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ids[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m pair[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     74\u001b[0m         newids\u001b[38;5;241m.\u001b[39mappend(idx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc2aa7-6a7a-4723-90b9-d00f9ea03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - cfg.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+cfg.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+cfg.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 3): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, target_token_ids=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=tcfg.lr_init, weight_decay=0.0)\n",
    "    # No weight decay to keep vectors on the unit hypersphere\n",
    "\n",
    "# Learning rate schedule without warmup\n",
    "def lr_lambda(current_iter):\n",
    "    # Cosine decay phase only\n",
    "    cosine_decay = 0.5 * (1 + math.cos(math.pi * current_iter / tcfg.max_iters))\n",
    "    return max(cosine_decay, tcfg.lr_final / tcfg.lr_init)\n",
    "        \n",
    "# Scheduler using cosine decay\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for iter in range(tcfg.max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % tcfg.eval_interval == 0 or iter == tcfg.max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, tcfg.micro_batch_size)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "    # setup for training\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # we can simulate a larget batch size by accumulating gradients over many micro batches\n",
    "    for micro_step in range(tcfg.grad_accum_steps):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', tcfg.micro_batch_size)\n",
    "        \n",
    "        # train\n",
    "        logits, loss = model(input_token_ids = xb, target_token_ids = yb)\n",
    "        \n",
    "        # accounting for the size of the micro batch\n",
    "        loss = loss / tcfg.grad_accum_steps\n",
    "        # adding the micro batch's loss to the total loss\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        \n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Cosine normalization for all Linear layers \n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                weight = module.weight\n",
    "                # Find the dimension that matches cfg.dim\n",
    "                dim_to_normalize = None\n",
    "                for dim, size in enumerate(weight.shape):\n",
    "                    if size == cfg.dim:\n",
    "                        dim_to_normalize = dim\n",
    "                        break\n",
    "                \n",
    "                if dim_to_normalize is not None:\n",
    "                    # Normalize the weights\n",
    "                    module.weight.data = cosine_norm(module.weight.data, dim=dim_to_normalize)\n",
    "                    # Calculate the norm along the specified dimension\n",
    "                    #norm = weight.data.norm(dim=dim_to_normalize)\n",
    "                    #assert torch.allclose(norm, torch.ones_like(norm), atol=1e-4), \\\n",
    "                        #f\"Weights in Linear layer {module} are not properly normalized.\"\n",
    "\n",
    "        for layer in model.layers:\n",
    "            # Now loop over all named parameters in each submodule\n",
    "            for name, param in layer.named_parameters():\n",
    "                # Check if the parameter's name matches 'a_A' or 'a_M'\n",
    "                if name in ['a_A', 'a_M']:\n",
    "                    # Apply absolute value to the parameter in place\n",
    "                    param.data = param.data.abs()\n",
    "                    #assert (param.data >= 0).all(), f\"Parameter {name} contains negative values.\"\n",
    "                \n",
    "                        \n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6552df-9608-4c17-b211-8bbe4b9db5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final absolute value check after training\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.ModuleList):\n",
    "        for submodule in module:\n",
    "            for name, param in submodule.named_parameters():\n",
    "                if name in ['a_A', 'a_M']:\n",
    "                    if (param.data < 0).any():\n",
    "                        print(f\"Warning: Parameter {name} contains negative values post-training.\")\n",
    "\n",
    "# Final cosine normalization check\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        weight = module.weight\n",
    "        dim_to_normalize = None\n",
    "        for dim, size in enumerate(weight.shape):\n",
    "            if size == cfg.dim:\n",
    "                dim_to_normalize = dim\n",
    "                break\n",
    "\n",
    "        if dim_to_normalize is not None:\n",
    "            norm = weight.data.norm(dim=dim_to_normalize)\n",
    "            if not torch.allclose(norm, torch.ones_like(norm), atol=1e-4):\n",
    "                print(f\"Warning: Weights in Linear layer {module} are not normalized post-training.\\n{norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77839e1-8c45-4e01-8514-bd259f31e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Dictionary to store parameters grouped by name\n",
    "params = defaultdict(list)\n",
    "scale_names = ['a_A', 'a_M', 's_qk', 's_u', 's_v', 's_z']\n",
    "\n",
    "# Collect all parameters\n",
    "for module in model.modules():\n",
    "    for name, param in module.named_parameters():\n",
    "        if name in scale_names:\n",
    "            params[name].append({\n",
    "                'shape': tuple(param.shape),\n",
    "                'mean': torch.mean(param).item(),\n",
    "                'std': torch.std(param).item()\n",
    "            })\n",
    "\n",
    "# Print results for each parameter type\n",
    "for param_name in scale_names:\n",
    "    if params[param_name]:\n",
    "        print(f\"\\n=== {param_name} Parameters ===\")\n",
    "        table_data = [[\n",
    "            i+1,\n",
    "            str(p['shape']),\n",
    "            f\"{p['mean']:.4f}\",\n",
    "            f\"{p['std']:.4f}\"\n",
    "        ] for i, p in enumerate(params[param_name])]\n",
    "        \n",
    "        print(tabulate(\n",
    "            table_data,\n",
    "            headers=['#', 'Shape', 'Mean', 'Std'],\n",
    "            tablefmt='simple',\n",
    "            floatfmt='.4f'\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import generate\n",
    "output = generate(\n",
    "    \"JULIET:\\nO Romeo, Romeo! wherefore art thou\", \n",
    "    model, \n",
    "    tokenizer, \n",
    "    temperature=0.01, # really weird that we've gotta use a pretty damn low temperature\n",
    "    max_gen_len = 128\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'models/{tcfg.model_name}', exist_ok=True)\n",
    "\n",
    "# saving model\n",
    "torch.save(model.state_dict(), f'models/{tcfg.model_name}/model.pth')\n",
    "\n",
    "# saving configs\n",
    "cfg_dict = asdict(cfg)\n",
    "with open(f'models/{tcfg.model_name}/model_config.json', 'w') as f:\n",
    "    json.dump(cfg_dict, f)\n",
    "tcfg_dict = asdict(tcfg)\n",
    "with open(f'models/{tcfg.model_name}/train_config.json', 'w') as f:\n",
    "    json.dump(tcfg_dict, f)\n",
    "\n",
    "print(f'model successfully saved to models/{tcfg.model_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140c2e2-02fe-4e6c-b745-7d26957e4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
