{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3509d342-f233-40d7-91d1-d4e865bd60ab",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "\n",
    "- [x] figure out how to implement memory_saver_div into the kv cache\n",
    "- [x] add dropout\n",
    "- [ ] train bigger version (longer context length?)\n",
    "- [ ] copy & paste into model.py\n",
    "- [ ] make a train.py\n",
    "- [ ] make a params.py\n",
    "- [ ] build colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model config\n",
    "from params import *\n",
    "\n",
    "# importing minLlama3\n",
    "from model import *\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c131b4ce-c393-4885-bd37-9a4651c7fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=12, n_heads=4, n_kv_heads=1, vocab_size=512, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=10000, max_batch_size=24, max_seq_len=512, device='cpu', dropout_rate=0.1)\n"
     ]
    }
   ],
   "source": [
    "params = ModelArgs()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985.088 K parameters\n",
      "Llama3(\n",
      "  (tok_embeddings): Embedding(512, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=128, out_features=512, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Llama3(params, tokenizer).to(params.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9dc2aa7-6a7a-4723-90b9-d00f9ea03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - params.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+params.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+params.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(params.device), y.to(params.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, targets=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "lr_init = 1e-2\n",
    "weight_decay = 0.02\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_init, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 2000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 100\n",
    "\n",
    "# Warmup setup\n",
    "warmup_iters = 50  # Number of warmup iterations\n",
    "warmup_factor = 1e-3  # Warmup factor (initial learning rate is multiplied by this factor)\n",
    "\n",
    "lr_final = 1e-5  # Minimum learning rate\n",
    "\n",
    "def lr_lambda(current_iter):\n",
    "    if current_iter < warmup_iters:\n",
    "        # Warmup phase\n",
    "        return warmup_factor + (1 - warmup_factor) * current_iter / warmup_iters\n",
    "    else:\n",
    "        # Cosine decay phase with minimum learning rate\n",
    "        decay_iters = max_iters - warmup_iters\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * (current_iter - warmup_iters) / decay_iters))\n",
    "        return max(cosine_decay, lr_final / lr_init)\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.000210, train loss 6.4700, val loss 6.4506, time elapsed: 3.92 seconds\n",
      "step 0100: lr 0.009983, train loss 3.0054, val loss 3.1842, time elapsed: 382.95 seconds\n",
      "step 0200: lr 0.009853, train loss 2.5898, val loss 2.8687, time elapsed: 769.07 seconds\n",
      "step 0300: lr 0.009597, train loss 2.3720, val loss 2.6845, time elapsed: 1161.52 seconds\n",
      "step 0400: lr 0.009222, train loss 2.1987, val loss 2.6030, time elapsed: 1545.14 seconds\n",
      "step 0500: lr 0.008737, train loss 2.1417, val loss 2.5857, time elapsed: 1924.22 seconds\n",
      "step 0600: lr 0.008156, train loss 2.0663, val loss 2.5644, time elapsed: 2301.25 seconds\n",
      "step 0700: lr 0.007493, train loss 1.9619, val loss 2.5837, time elapsed: 2680.06 seconds\n",
      "step 0800: lr 0.006765, train loss 1.9134, val loss 2.5494, time elapsed: 3057.81 seconds\n",
      "step 0900: lr 0.005992, train loss 1.8540, val loss 2.5846, time elapsed: 3436.21 seconds\n",
      "step 1000: lr 0.005193, train loss 1.7917, val loss 2.6647, time elapsed: 3831.66 seconds\n",
      "step 1100: lr 0.004389, train loss 1.7110, val loss 2.6274, time elapsed: 4213.83 seconds\n",
      "step 1200: lr 0.003601, train loss 1.6439, val loss 2.6857, time elapsed: 4596.22 seconds\n",
      "step 1300: lr 0.002849, train loss 1.5828, val loss 2.7742, time elapsed: 4978.64 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, targets\u001b[38;5;241m=\u001b[39myb)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Update the learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/minLlama3/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/minLlama3/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', params.max_batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, params.max_batch_size)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Ah, Romeo! thy hardness shall have heavy fire,\n",
      "That thou shalt swear thy country and thy brother,\n",
      "Shall I be patient to thy grief again;\n",
      "And I for thee here a barge for thee,\n",
      "Look, then thou hast thy side both being all,\n",
      "But thou hast won thee to thy hand and me?\n",
      "O she be not thy shoulder-black deserved thoughts,\n",
      "Or I with banishment with some axe,\n",
      "Shall I ever will have the time a happy days\n",
      "And turn thy love, and my good night have shorted\n",
      "That I am in a golden side.\n",
      "\n",
      "ROMEO:\n",
      "I am a poison of this fair is a soldier,\n",
      "And I am gone with testimonies with my head.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "A thousand times are too dear fortune's death!\n",
      "\n",
      "ROMEO:\n",
      "A fair sir, that is thy device that way\n",
      "Had on thy lives and meditating with my hand;\n",
      "The trumpet, book in thy daughter shall be gone.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "And thou love me, and first \n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"JULIET:\\nO Romeo, Romeo! wherefore art thou R\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "params_dict = asdict(params)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(params_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140c2e2-02fe-4e6c-b745-7d26957e4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
