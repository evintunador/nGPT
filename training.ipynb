{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "# running it shouldn't break anything for u\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv')\n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model config\n",
    "from config import *\n",
    "\n",
    "# importing N-GPT\n",
    "from model import cosine_norm, Model\n",
    "\n",
    "# imports for the tokenizer\n",
    "import pickle\n",
    "from tokenizer.tokenizer import BPE_Tokenizer\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "import math\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=128, device=None, max_seq_len=384, theta=10000, vocab_len=2048, num_layers=8, num_heads=4, mlp_hidden_mult=4)\n",
      "TrainConfig(model_name='N-GPT_2m', micro_batch_size=4, grad_accum_steps=16, max_iters=1000, eval_interval=100, beta1=0.9, beta2=0.95, epsilon=1e-08, lr_init=0.0005, lr_final=1e-08)\n"
     ]
    }
   ],
   "source": [
    "cfg = ModelConfig()\n",
    "print(cfg)\n",
    "tcfg = TrainConfig()\n",
    "print(tcfg)\n",
    "\n",
    "# size options are 512, 1024 and 2048\n",
    "with open(f'tokenizer/models/{cfg.vocab_len - 3}.model', 'rb') as f:\n",
    "        tokenizer_data = pickle.load(f)\n",
    "tokenizer = BPE_Tokenizer(tokenizer_data['merges']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1844560 parameters\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (s_qk): Scale()\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (alpha_A): Scale()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=128, out_features=341, bias=False)\n",
      "        (Wgate): Linear(in_features=128, out_features=341, bias=False)\n",
      "        (Wdown): Linear(in_features=341, out_features=128, bias=False)\n",
      "        (s_u): Scale()\n",
      "        (s_v): Scale()\n",
      "      )\n",
      "      (alpha_M): Scale()\n",
      "    )\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=2048, bias=False)\n",
      "  (s_z): Scale()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(f'{model.get_num_params()} parameters')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7629a97c-5898-4dba-b602-fd26721ff5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dc2aa7-6a7a-4723-90b9-d00f9ea03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - cfg.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+cfg.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+cfg.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 3): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, target_token_ids=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=tcfg.lr_init, weight_decay=0.0)\n",
    "    # No weight decay to keep vectors on the unit hypersphere\n",
    "\n",
    "# Learning rate schedule without warmup\n",
    "def lr_lambda(current_iter):\n",
    "    # Cosine decay phase only\n",
    "    cosine_decay = 0.5 * (1 + math.cos(math.pi * current_iter / tcfg.max_iters))\n",
    "    return max(cosine_decay, tcfg.lr_final / tcfg.lr_init)\n",
    "        \n",
    "# Scheduler using cosine decay\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.000500, train loss 39.0102, val loss 38.8883, time elapsed: 0.00 seconds\n",
      "step 0100: lr 0.000488, train loss 7.0910, val loss 7.0731, time elapsed: 164.32 seconds\n",
      "step 0200: lr 0.000452, train loss 6.5016, val loss 6.5262, time elapsed: 324.85 seconds\n",
      "step 0300: lr 0.000397, train loss 6.1771, val loss 6.2195, time elapsed: 524.57 seconds\n",
      "step 0400: lr 0.000327, train loss 5.9875, val loss 6.0033, time elapsed: 800.14 seconds\n",
      "step 0500: lr 0.000250, train loss 5.8212, val loss 5.8767, time elapsed: 1073.34 seconds\n",
      "step 0600: lr 0.000173, train loss 5.7313, val loss 5.8072, time elapsed: 1345.06 seconds\n",
      "step 0700: lr 0.000103, train loss 5.6668, val loss 5.7730, time elapsed: 1617.06 seconds\n",
      "step 0800: lr 0.000048, train loss 5.5971, val loss 5.7359, time elapsed: 1889.12 seconds\n",
      "step 0900: lr 0.000012, train loss 5.6274, val loss 5.6946, time elapsed: 2161.36 seconds\n",
      "step 0999: lr 0.000000, train loss 5.6317, val loss 5.7475, time elapsed: 2437.86 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for iter in range(tcfg.max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % tcfg.eval_interval == 0 or iter == tcfg.max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, tcfg.micro_batch_size)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "    # setup for training\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # we can simulate a larget batch size by accumulating gradients over many micro batches\n",
    "    for micro_step in range(tcfg.grad_accum_steps):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', tcfg.micro_batch_size)\n",
    "        \n",
    "        # train\n",
    "        logits, loss = model(input_token_ids = xb, target_token_ids = yb)\n",
    "        \n",
    "        # accounting for the size of the micro batch\n",
    "        loss = loss / tcfg.grad_accum_steps\n",
    "        # adding the micro batch's loss to the total loss\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        \n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Apply cosine normalization & absolute value constraints after optimization step\n",
    "    model.enforce_constraints()  \n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "811c7336-62fb-4d0d-a3da-58378d65c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1167, 0.1951, 0.1287, 0.1083, 0.1109, 0.1502, 0.1045, 0.1445, 0.1062,\n",
      "         0.1298, 0.1070, 0.1279, 0.1209, 0.1164, 0.1321, 0.1170, 0.0900, 0.1096,\n",
      "         0.1366, 0.1098, 0.1176, 0.1111, 0.1199, 0.1226, 0.1073, 0.1137, 0.1615,\n",
      "         0.2416, 0.1228, 0.1286, 0.1177, 0.1306, 0.1100, 0.1014, 0.2246, 0.1477,\n",
      "         0.1544, 0.1089, 0.1097, 0.1163, 0.2003, 0.1139, 0.2482, 0.1566, 0.1577,\n",
      "         0.1065, 0.0985, 0.1066, 0.1257, 0.2109, 0.1540, 0.1266, 0.0994, 0.1061,\n",
      "         0.1442, 0.1269, 0.1469, 0.0940, 0.1620, 0.1511, 0.1205, 0.1216, 0.2291,\n",
      "         0.1227, 0.1070, 0.1037, 0.1172, 0.1163, 0.2006, 0.1323, 0.1226, 0.1115,\n",
      "         0.1220, 0.1071, 0.1381, 0.1233, 0.1568, 0.0963, 0.2137, 0.0963, 0.1283,\n",
      "         0.1736, 0.1019, 0.1969, 0.1122, 0.1587, 0.1104, 0.1092, 0.1429, 0.1119,\n",
      "         0.1074, 0.1081, 0.1091, 0.1165, 0.1104, 0.1822, 0.1078, 0.1122, 0.1100,\n",
      "         0.1046, 0.1160, 0.2186, 0.1992, 0.1038, 0.1265, 0.1115, 0.1284, 0.1844,\n",
      "         0.1335, 0.0992, 0.1120, 0.1429, 0.1013, 0.1192, 0.1226, 0.1935, 0.1529,\n",
      "         0.1191, 0.1231, 0.1245, 0.1199, 0.1067, 0.1238, 0.1174, 0.1412, 0.1302,\n",
      "         0.1022, 0.1050]], device='mps:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='mps:0', grad_fn=<NormBackward1>)\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='mps:0', grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# checking to make sure the absolute value-ing worked\n",
    "print(model.layers[0].alpha_A.s.data)\n",
    "# checking to make sure the cosine normalization worked\n",
    "print(model.layers[0].mlp.Wup.weight.norm(dim=1))\n",
    "print(model.token_embedder.weight.norm(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou art not in your parged.\n",
      "\n",
      "KING RICHARD II:\n",
      "I will be feton, that we will be damn'd,\n",
      "For my lord, that now feton to our bavour.\n",
      "\n",
      "GLOUCESTER:\n",
      "My lord to the barget with his prages,\n",
      "I will be davestes, I will be feton of your parging.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "I will be davour'd the pavour,\n",
      "That now in your got'd a bay'd in his birps.\n",
      "\n",
      "KING RICHARD II:\n",
      "Tavest of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from inference import generate\n",
    "output = generate(\n",
    "    \"JULIET:\\nO Romeo, Romeo! wherefore art thou\", \n",
    "    model, \n",
    "    tokenizer, \n",
    "    temperature=0.01, # really weird that we've gotta use a pretty damn low temperature\n",
    "    max_gen_len = 128\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully saved to models/N-GPT_2m/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f'models/{tcfg.model_name}', exist_ok=True)\n",
    "\n",
    "# saving model\n",
    "torch.save(model.state_dict(), f'models/{tcfg.model_name}/model.pth')\n",
    "\n",
    "# saving configs\n",
    "cfg_dict = asdict(cfg)\n",
    "with open(f'models/{tcfg.model_name}/model_config.json', 'w') as f:\n",
    "    json.dump(cfg_dict, f)\n",
    "tcfg_dict = asdict(tcfg)\n",
    "with open(f'models/{tcfg.model_name}/train_config.json', 'w') as f:\n",
    "    json.dump(tcfg_dict, f)\n",
    "\n",
    "print(f'model successfully saved to models/{tcfg.model_name}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
