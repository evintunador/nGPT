{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WUJqR8FeXSU"
   },
   "source": [
    "# Core Ideas of Nvidia's N-GPT\n",
    "\n",
    "This notebook guide is designed for people who are already confident with modern transformers (ex. Llama3). If you are a complete beginner, check out my [Llama3 tutorial](https://colab.research.google.com/drive/10BKvPomnVVZw7UAT3wOaaPBdvfMEvOOY?usp=sharing).\n",
    "for an accelerated introduction designed for those who already understand basic math concepts like matrix multiplication or [Andrej Karpathy's \"Neural Networks: Zero to Hero\" course](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&si=8Z9BUgdFAnGBo71c) for those who need to start from scratch. The purpose of this guide is to provide intuition behind the architecture choices implemented in [Nvidia's N-GPT](https://arxiv.org/abs/2410.01131v1) without getting into the particulars (for that, read model.py). \n",
    "\n",
    "Check out the YouTube video where i walk through the paper:\n",
    "\\[\\!\\[ERROR DISPLAYING IMAGE, CLICK HERE FOR VIDEO](https://img.youtube.com/vi/lZj8F6EspVU/0.jpg)](https://www.youtube.com/watch?v=lZj8F6EspVU)\n",
    "\n",
    "**Note:** It's very easy to convince yourself that you understand something after watching a youtube video about it, but chances are you don't actually understand unless you can write out the math and code it from scratch on your own. I highly recommend doing so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXFTnCXm_Dfj"
   },
   "source": [
    "### Setup stuff\n",
    "\n",
    "imports & hyperparameters & whatnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JOHHIHcjeWzN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xEje5kPPeW1Y"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    dim: int = 8 # the model's embedding dimension\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "        # defaults to best available GPU/CPU\n",
    "    max_seq_len: int = 5 # maximum number of tokens in the context\n",
    "    theta: float = 10_000. # RoPE hyperparameter; 10_000 is the most common choice\n",
    "    vocab_len: int = 2048 # options are 512, 1024, 2048\n",
    "    num_layers: int = 4 # number of layers in the model\n",
    "    num_heads: int = 2 # number of heads in the multi-head attention mechanism\n",
    "    mlp_hidden_mult: float = 1.5 # how wide the hidden dimension of the MLP should be compared to dim\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 3 \n",
    "    max_iters: int = 100 # total number of batches to run over the course of training\n",
    "    # AdamW Hyperparameters https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95\n",
    "    epsilon: float = 1e-8\n",
    "    # N-GPT disables weight-decay in the optimizer since it would move vectors off of the unit-hypersphere\n",
    "    weight_decay: float = 0.0 \n",
    "    # Maximum and minimum learning rates during annealing\n",
    "    lr_init: float = 5e-3 # N-GPT does NOT need to use learning rate warmup bc training is so stable\n",
    "    lr_final: float = 1e-5\n",
    "\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-nuFaBitYeo"
   },
   "source": [
    "### The broad ideas\n",
    "<a id='2'></a>\n",
    "The two key innovations of this architecture and the result of implementing them, as stated by the authors, are as follows\n",
    "\n",
    "![picture](images/key_contributions.png)\n",
    "\n",
    "Putting vectors onto the unit hypersphere in classification settings has been popular for quite awhile in the computer vision field, for some reason especially so with medical imaging. Transformers themselves being in-context optimizers has been a hot topic of debate for in-context learning research, but from what i've read it does seem to have merit at least in some contexts, and what they did here was design the model in a manner that embraces this idea\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN6ycHjMeJbJ"
   },
   "source": [
    "### Cosine Normalization\n",
    "\n",
    "The idea here is that whereas traditional GPT models use either [Layer Norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) or [RMS Norm](https://arxiv.org/abs/1910.07467) and do so usually only on the residual stream vectors, here we'll be using cosine normalization which both 1) does \"more\" normalization and 2) will also be applied to the weight matrices along their embedding-length dimension.\n",
    "\n",
    "LayerNorm and RMSNorm are relatively similar, both placing vectors onto the hypersphere of radius $\\sqrt{dim}$, with the main difference being that the former also centers those spheres around the origin while the latter does not, effectively meaning it's putting each vector onto the surface of different hyperspheres. See my in-depth video explanation of what LayerNorm does to vectors [here](https://youtu.be/vlgLbQtL1RE)\n",
    "\n",
    "Cosine normalization on the other hand places all vectors onto the same hypersphere centered at the origin with radius $1$, aka the unit-hypersphere. In the code below we can see it's relatively simple; just divide the vector by its own norm\n",
    "\n",
    "But how do we actually go about ensuring that all of our weight matrices have been cosine-normalized? The two implementations I'm aware of are that it can either be done in the forward pass before using said weights, or by forcing it after every iteration of gradient descent in the training loop. For efficiency during training I'm not sure it matters other than maybe the iteration through nn.Modules might be slower, and for inference it's definitely faster to not have to call the cosine_norm function every single forward pass. Over in `training.ipynb` I've gone the route of doing it during the training loop right after the gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BItItwY2eW3m"
   },
   "outputs": [],
   "source": [
    "def cosine_norm(x: torch.Tensor, dim=-1) -> torch.Tensor:\n",
    "    # calculate the magnitude of the vectors\n",
    "    norm = torch.norm(x, p=2, dim=dim, keepdim=True).clamp(min=1e-6)\n",
    "    # divide by the magnitude to place on the unit hypersphere\n",
    "    return x / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mknMcCmEfmd-",
    "outputId": "d19ba24c-ddd8-43f1-bbc9-62294c218045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual state norms: tensor([[[2.6883],\n",
      "         [2.4465],\n",
      "         [2.0404],\n",
      "         [1.9038],\n",
      "         [3.4173]],\n",
      "\n",
      "        [[1.8770],\n",
      "         [2.3548],\n",
      "         [2.3282],\n",
      "         [3.9562],\n",
      "         [3.0683]],\n",
      "\n",
      "        [[3.7325],\n",
      "         [2.2250],\n",
      "         [2.1887],\n",
      "         [0.9713],\n",
      "         [2.4724]]], device='mps:0')\n",
      "weights matrix norms: tensor([[2.6406, 1.3802, 3.5841, 2.0801, 2.0012, 1.3385, 2.4916, 2.3445, 3.0002,\n",
      "         2.3255, 2.5163, 2.0334]], device='mps:0')\n",
      "Normalized residual state norms: tensor([[[1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000]]], device='mps:0')\n",
      "Normalized weights matrix norms: tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Example usage of cosine_norm on a residual state tensor and a weights matrix\n",
    "residual_state = torch.randn(tcfg.batch_size, cfg.max_seq_len, cfg.dim, device=cfg.device)\n",
    "weights_matrix = torch.randn(cfg.dim, int(cfg.dim * cfg.mlp_hidden_mult), device=cfg.device)\n",
    "\n",
    "print(\"residual state norms:\", torch.norm(residual_state, p=2, dim=-1, keepdim=True).clamp(min=1e-6))\n",
    "print(\"weights matrix norms:\", torch.norm(weights_matrix, p=2, dim=0, keepdim=True).clamp(min=1e-6))\n",
    "\n",
    "# Normalize the residual state tensor\n",
    "normalized_residual_state = cosine_norm(residual_state)\n",
    "\n",
    "# Normalize the weights matrix along the embedding dimension (dim=0)\n",
    "normalized_weights_matrix = cosine_norm(weights_matrix, dim=0)\n",
    "\n",
    "print(\"Normalized residual state norms:\", torch.norm(normalized_residual_state, p=2, dim=-1, keepdim=True).clamp(min=1e-6))\n",
    "print(\"Normalized weights matrix norms:\", torch.norm(normalized_weights_matrix, p=2, dim=0, keepdim=True).clamp(min=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaaYqS8oiE4Q"
   },
   "source": [
    "### 2b. Interpreting Matmuls\n",
    "This is what matmuls look like with and without cosine normalization. Notice in the code example below how the former is technically unbounded, although realistically it clusters around 1 with some expected variance, while the latter is bounded in $[-1,1]$. Dot-products between two vectors on the unit-hypersphere can be interpreted as performing cosine similarity between them where a value of $-1$ corresponds to completely opposite vectors, $0$ corresponds to orthogonal, and $1$ is what you would get if they're the exact same vector.\n",
    "\n",
    "Here's the traditional cosine normalization formula for non-normalized vectors where $\\cdot$ denotes dotproduct, $||a||$ denotes the magnitude of $a$, and $\\times$ is scalar multiplication:\n",
    "$$ \\frac{a \\cdot b}{||a||\\times||b||} $$\n",
    "\n",
    "And since a cosine normalized vector $a$ has already been divided by $||a||$, the cosine similarity formula simplifies down to\n",
    "$$a\\cdot b$$\n",
    "\n",
    "This property might potentially open up interesting future avenues for interpretability researach since we can now understand matmuls as checking the similarity between vectors in the input tensor and vectors in the weight tensor. What vectors might the model think it important to compare to the input when deciding how to edit the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkjlOI2oh4u7",
    "outputId": "198523d6-e8d1-473d-b6a8-6cb864c00174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul value range: -6.9036760330200195 7.527524948120117\n",
      "normalized_matmul value range: -0.7024921178817749 0.8642598986625671\n"
     ]
    }
   ],
   "source": [
    "matmul = residual_state @ weights_matrix\n",
    "normalized_matmul = normalized_residual_state @ normalized_weights_matrix\n",
    "\n",
    "# comparing the value ranges of matmul & normalized_matmul\n",
    "print(\"matmul value range:\", matmul.min().item(), matmul.max().item())\n",
    "print(\"normalized_matmul value range:\", normalized_matmul.min().item(), normalized_matmul.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53BqJ9NzeAUu"
   },
   "source": [
    "### 2c. Transformers as variable-metric optimizers\n",
    "<a id='4'></a>\n",
    "\n",
    "The idea here is to interpret each residual connection (whether it be attention mechanism or multi-layer perceptron) as itself calculating a gradient in the direction of the final to-be-outputted token. Prior interpretations of this sort that I'm aware of have focused more on understanding transformers through this lens, but this work actually attempts to shape these residual connections to better perform this function.\n",
    "\n",
    "The traditional residual connection update equation looks something like:\n",
    "$$ h_{l+1} = h_l + h_l^A$$\n",
    "$$ h_{l+1} = h_l + h_l^M$$\n",
    "where $h_l$ is the hidden state at the $l$'th layer and $h_l^A = \\text{Attention}(\\text{RMSNorm}(h_l))$ and $h_l^M = \\text{MLP}(\\text{RMSNorm}(h_l))$ denote the output of the attention and MLP respectively.\n",
    "\n",
    "The natural question to ask once viewing residual connections this way is: how large of a step are these models taking in the direction of the gradient? Prior GPT models would have to implicitly incorporate this decision into the attention mechanism or MLP itself, so if that function could instead be separated out then it should leave more of the model's regular parameters to do the gradient part. When we re-phrase the goal in terms of variable-metric optimizers and also make our cosine-normalization adjustment, we get\n",
    "$$ h_{l+1} = h_l + a_A * g_A $$\n",
    "$$ h_{l+1} = h_l + a_M * g_M $$\n",
    "where $g_A = h_l^A - h_l$ and $g_M = h_l^M - h_l$ can be interpreted as the gradients from their respective modules, $*$ denotes entry-wise multiplication, and $a_A$ and $a_M$ are parameters determining the size of the gradient steps for their respective modules, analagous to $\\eta$ in actual gradient descent. The authors call them \"eigen\" learning rate vectors, a name based in etymology rather than any relation to eigenvalues and eigenvectors, which I think is very confusing and a bad choice on their part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WjgNvJHp7J5l"
   },
   "outputs": [],
   "source": [
    "class MiniTransformerLayer(nn.Module):\n",
    "    def __init__(self, dim, device):\n",
    "        super().__init__()\n",
    "        # let's pretend for a second that this is an entire multi-layer perceptrion instead of a single linear layer\n",
    "        self.MLP = nn.Linear(dim,  dim, bias=False, device=device)\n",
    "\n",
    "        # and now our eigen learning rate vector, which we initialize to a value of a_M_scale for all entries\n",
    "        self.a_M = nn.Parameter(torch.ones(dim, device=device))\n",
    "\n",
    "    def forward(self, h_l: torch.Tensor) -> torch.Tensor:\n",
    "        # first run the actual multi-layer perceptron\n",
    "        h_M = self.MLP(h_l)\n",
    "        # finally do the actual residual layer update\n",
    "        h_lplus1 = h_l + self.a_M * (h_M - h_l)\n",
    "        return h_lplus1\n",
    "\n",
    "h_l = torch.randn(tcfg.batch_size, cfg.max_seq_len, cfg.dim, device=cfg.device)\n",
    "mini_transformer_layer = MiniTransformerLayer(cfg.dim, cfg.device)\n",
    "h_lplus1 = mini_transformer_layer(h_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wccyOLWQjyQN"
   },
   "source": [
    "### 2d. Scaling parameters\n",
    "<a id='5'></a>\n",
    "\n",
    "These edits are all well and good, but the astute observer might have been concerned when I showed earlier in section 2b that the distributions of the outputs of our operations are very different from what they would be without all these edits. I won't get too deep into the issues presented here (see pages 5 and 12 of the paper for more), but what we're about to do here essentially amounts to controlling the learning rate of a couple specific key parameters in the model without affecting all of the others.\n",
    "\n",
    "Here's how the authors describe the methodology with the example of $a_A$ and $a_M$. It took me a few reads to understand what's going on since I had no code to look at, but hopefully my replication will help make the process clear for you.\n",
    "\n",
    "![alt text](images/scaling_parameters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "2eL1OwBVjxiC"
   },
   "outputs": [],
   "source": [
    "class MiniTransformerLayer(nn.Module):\n",
    "    def __init__(self, dim, device):\n",
    "        super().__init__()\n",
    "        # let's pretend for a second that this is an entire multi-layer perceptrion instead of a single linear layer\n",
    "        self.MLP = nn.Linear(dim,  dim, bias=False, device=device)\n",
    "\n",
    "        # define our scaling parameters\n",
    "        self.a_M_scale = 1. / math.sqrt(dim)\n",
    "        self.a_M_init = 1.\n",
    "\n",
    "        # and now our eigen learning rate vector, which we initialize to a value of a_M_scale for all entries\n",
    "        self.a_M = nn.Parameter(torch.ones(dim, device=device) * self.a_M_scale)\n",
    "\n",
    "    def forward(self, h_l: torch.Tensor) -> torch.Tensor:\n",
    "        # first run the actual multi-layer perceptron\n",
    "        h_M = self.MLP(h_l)\n",
    "        # then calculate our effective scaling parameter\n",
    "        effective_a_M = self.a_M * (self.a_M_init / self.a_M_scale)\n",
    "        # finally do the actual residual layer update\n",
    "        h_lplus1 = h_l + effective_a_M * (h_M - h_l)\n",
    "        return h_lplus1\n",
    "\n",
    "h_l = torch.randn(tcfg.batch_size, cfg.max_seq_len, cfg.dim, device=cfg.device)\n",
    "mini_transformer_layer = MiniTransformerLayer(cfg.dim, cfg.device)\n",
    "h_lplus1 = mini_transformer_layer(h_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aight, i guess now go read the actual code to see all the specifics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
